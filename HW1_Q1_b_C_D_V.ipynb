{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mgUf8vLb01z8",
        "outputId": "bd0079ec-86f7-463d-c89b-582e96051b94"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "xmEcx8Wu1XPw"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/drive/MyDrive/HW1_NNF/')  # Add the directory containing the file to the Python path\n",
        "import SeprationIndex  # Import your module"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing essential libraries and defining flatten transformation"
      ],
      "metadata": {
        "id": "reGFoZyP1xWZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from SeprationIndex import Kalhor_SeparationIndex\n",
        "\n",
        "# Define a custom transformation to flatten the images\n",
        "class FlattenTransform:\n",
        "    def __call__(self, image):\n",
        "        return image.view(-1)\n",
        "\n",
        "# Define the transformations\n",
        "transform = transforms.Compose([transforms.ToTensor(), FlattenTransform()])\n"
      ],
      "metadata": {
        "id": "FJA6CstSWKiv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "spiliting training dataset \\\\\n",
        "finding instance for its data \\\\\n",
        "calculating SI"
      ],
      "metadata": {
        "id": "ffhu3P2P180o"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.utils.data import ConcatDataset\n",
        "#Download train dataset from mnist dataset\n",
        "dataset  = torchvision.datasets.MNIST(root='./data',train=True,transform=transform,download=True)\n",
        "\n",
        "\n",
        "num_samples_per_class = 1000\n",
        "\n",
        "\n",
        "total_samples = num_samples_per_class * 10\n",
        "\n",
        "\n",
        "class_indices = dataset.targets.unique()\n",
        "datasets_per_class = []\n",
        "for class_index in class_indices:\n",
        "    class_samples = [i for i, label in enumerate(dataset.targets) if label == class_index]\n",
        "    selected_samples = class_samples[:num_samples_per_class]\n",
        "    selected_dataset = torch.utils.data.Subset(dataset, selected_samples)\n",
        "    datasets_per_class.append(selected_dataset)\n",
        "\n",
        "dataset = ConcatDataset(datasets_per_class)\n",
        "\n",
        "# Get the data and labels from the concatenated dataset\n",
        "data_concatenated = torch.stack([dataset[i][0] for i in range(len(dataset))])\n",
        "labels_concatenated = torch.tensor([dataset[i][1] for i in range(len(dataset))]).view(-1, 1)\n",
        "\n",
        "# Create an instance of Kalhor_SeparationIndex\n",
        "instance_concatenated = Kalhor_SeparationIndex(data_concatenated, labels_concatenated)\n",
        "\n",
        "# Compute the Separation Index for the concatenated dataset\n",
        "si_concatenated = instance_concatenated.si()\n",
        "\n",
        "# Print the Separation Index for the concatenated dataset\n",
        "print(\"Separation Index for the Concatenated Dataset:\", si_concatenated.detach().cpu().numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ylFf3G8UYpme",
        "outputId": "9a58ec25-f165-4f2d-9b5d-3f3205b91e8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data/MNIST/raw/train-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 9912422/9912422 [00:00<00:00, 86378165.23it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data/MNIST/raw/train-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 28881/28881 [00:00<00:00, 11537831.59it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/train-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw/t10k-images-idx3-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 1648877/1648877 [00:00<00:00, 28376496.69it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-images-idx3-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz\n",
            "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|██████████| 4542/4542 [00:00<00:00, 3664972.83it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Extracting ./data/MNIST/raw/t10k-labels-idx1-ubyte.gz to ./data/MNIST/raw\n",
            "\n",
            "Separation Index for the Concatenated Dataset: 0.952\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate high-order SI with order 2\n",
        "order = 2\n",
        "high_si = instance_concatenated.high_order_si(order)\n",
        "print('High-order Separation Index (Order {}):'.format(order), high_si.detach().cpu().numpy())\n",
        "\n",
        "# Calculate high-order soft SI with order 3\n",
        "order = 3\n",
        "soft_si = instance_concatenated.soft_order_si(order)\n",
        "print('High-order Soft Separation Index (Order {}):'.format(order), soft_si.detach().cpu().numpy())\n",
        "\n",
        "# Calculate center-based SI\n",
        "csi = instance_concatenated.center_si()\n",
        "print('Center-based Separation Index:', csi.detach().cpu().numpy())\n",
        "\n",
        "# Calculate anti-SI with order 2\n",
        "order = 2\n",
        "anti_si = instance_concatenated.anti_si(order)\n",
        "print('Anti Separation Index (Order {}):'.format(order), anti_si.detach().cpu().numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JY43JK-dhm5a",
        "outputId": "16a35578-3d51-44b5-f510-546bd7e1e944"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "High-order Separation Index (Order 2): 0.9136\n",
            "High-order Soft Separation Index (Order 3): 0.9361333\n",
            "Center-based Separation Index: 0.8038\n",
            "Anti Separation Index (Order 2): 0.0263\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Calculating SI for test dataset"
      ],
      "metadata": {
        "id": "myl0-nXg2dEw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MNIST test dataset\n",
        "mnist_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "# Get the data and labels from the test set\n",
        "data = torch.stack([mnist_dataset[i][0] for i in range(len(mnist_dataset))])\n",
        "labels = torch.tensor([mnist_dataset[i][1] for i in range(len(mnist_dataset))])\n",
        "labels = labels.view(-1, 1)\n",
        "\n",
        "# Create an instance of Kalhor_SeparationIndex\n",
        "instance = Kalhor_SeparationIndex(data, labels)\n",
        "\n",
        "# Compute the Separation Index\n",
        "si = instance.si()\n",
        "\n",
        "# Print the Separation Index\n",
        "print(\"Separation Index:\", si.detach().cpu().numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KyEvs0rY4jyI",
        "outputId": "62b3ca7d-d68a-4258-cd58-5cf566ad0d24"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Separation Index: 0.9558\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Yylt9Scxum-i",
        "outputId": "744f0d43-7cd0-4770-f484-c5a833b0fecc"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "High-order Separation Index (Order 2): 0.9188\n",
            "High-order Soft Separation Index (Order 3): 0.9410333\n",
            "Center-based Separation Index: 0.8229\n",
            "Anti Separation Index (Order 2): 0.0235\n"
          ]
        }
      ],
      "source": [
        "# Calculate high-order SI with order 2\n",
        "order = 2\n",
        "high_si = instance.high_order_si(order)\n",
        "print('High-order Separation Index (Order {}):'.format(order), high_si.detach().cpu().numpy())\n",
        "\n",
        "# Calculate high-order soft SI with order 3\n",
        "order = 3\n",
        "soft_si = instance.soft_order_si(order)\n",
        "print('High-order Soft Separation Index (Order {}):'.format(order), soft_si.detach().cpu().numpy())\n",
        "\n",
        "# Calculate center-based SI\n",
        "csi = instance.center_si()\n",
        "print('Center-based Separation Index:', csi.detach().cpu().numpy())\n",
        "\n",
        "# Calculate anti-SI with order 2\n",
        "order = 2\n",
        "anti_si = instance.anti_si(order)\n",
        "print('Anti Separation Index (Order {}):'.format(order), anti_si.detach().cpu().numpy())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------------------------بخش ج---------------------------------\\\n"
      ],
      "metadata": {
        "id": "cg8IsM7k91cl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9QWHkB4Zy1QC"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "class AlexNet(nn.Module):\n",
        "    def __init__(self, num=10):\n",
        "        super(AlexNet, self).__init__()\n",
        "        self.feature = nn.Sequential(\n",
        "            nn.Conv2d(1, 32, kernel_size=5, stride=1, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(32, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d( kernel_size=2, stride=2),\n",
        "            nn.Conv2d(64, 96, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(96, 64, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Conv2d(64, 32, kernel_size=3, padding=1),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.MaxPool2d( kernel_size=2, stride=1),\n",
        "        )\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(32*12*12,2048),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Dropout(),\n",
        "            nn.Linear(2048,1024),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(1024,num),\n",
        "\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "        x = self.feature(x)\n",
        "        x = x.view(-1,32*12*12)\n",
        "        x = self.classifier(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "reading the saved weight and using them on alexnet \\\n",
        "spiliting 500 data from each classes: 5000 data from training dataset \\"
      ],
      "metadata": {
        "id": "XAOgrXKC2zCA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9rv2fOanvyyE",
        "outputId": "511fe57d-c931-4ef6-98e7-7fd36d87e50b"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AlexNet(\n",
              "  (feature): Sequential(\n",
              "    (0): Conv2d(1, 32, kernel_size=(5, 5), stride=(1, 1), padding=(1, 1))\n",
              "    (1): ReLU(inplace=True)\n",
              "    (2): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (3): ReLU(inplace=True)\n",
              "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "    (5): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (6): ReLU(inplace=True)\n",
              "    (7): Conv2d(96, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (8): ReLU(inplace=True)\n",
              "    (9): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (10): ReLU(inplace=True)\n",
              "    (11): MaxPool2d(kernel_size=2, stride=1, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (classifier): Sequential(\n",
              "    (0): Dropout(p=0.5, inplace=False)\n",
              "    (1): Linear(in_features=4608, out_features=2048, bias=True)\n",
              "    (2): ReLU(inplace=True)\n",
              "    (3): Dropout(p=0.5, inplace=False)\n",
              "    (4): Linear(in_features=2048, out_features=1024, bias=True)\n",
              "    (5): ReLU(inplace=True)\n",
              "    (6): Linear(in_features=1024, out_features=10, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 6
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms\n",
        "from SeprationIndex import Kalhor_SeparationIndex\n",
        "# Define a custom transformation to flatten the images and normalize them (if required):\n",
        "class FlattenTransform:\n",
        "    def __call__(self, image):\n",
        "        return image.view(-1)\n",
        "\n",
        "# Define the transformations\n",
        "transform = transforms.Compose([transforms.ToTensor()])\n",
        "\n",
        "from torch.utils.data import ConcatDataset\n",
        "#Download train dataset from mnist dataset\n",
        "dataset  = torchvision.datasets.MNIST(root='./data',train=True,transform=transform,download=True)\n",
        "\n",
        "\n",
        "num_samples_per_class = 500\n",
        "\n",
        "\n",
        "total_samples = num_samples_per_class * 10\n",
        "\n",
        "\n",
        "class_indices = dataset.targets.unique()\n",
        "datasets_per_class = []\n",
        "for class_index in class_indices:\n",
        "    class_samples = [i for i, label in enumerate(dataset.targets) if label == class_index]\n",
        "    selected_samples = class_samples[:num_samples_per_class]\n",
        "    selected_dataset = torch.utils.data.Subset(dataset, selected_samples)\n",
        "    datasets_per_class.append(selected_dataset)\n",
        "\n",
        "dataset = ConcatDataset(datasets_per_class)\n",
        "\n",
        "# Get the data and labels from the concatenated dataset\n",
        "data = torch.stack([dataset[i][0] for i in range(len(dataset))])\n",
        "labels = torch.tensor([dataset[i][1] for i in range(len(dataset))]).view(-1, 1)\n",
        "labels = labels.view(-1, 1)  # Ensure the labels have the correct shape\n",
        "\n",
        "# Load your trained model checkpoint\n",
        "model_checkpoint = torch.load(\"/content/drive/MyDrive/HW1_NNF/saved_model.pth\")\n",
        "\n",
        "# Create the model architecture\n",
        "# Replace 'YourModelClass' with the actual class used to define your model\n",
        "model = AlexNet()\n",
        "model.load_state_dict(model_checkpoint)\n",
        "model.eval()  # Set the model to evaluation mode\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "extracting last layer's features"
      ],
      "metadata": {
        "id": "-atlBgOV3fow"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a forward hook function to capture features before the fully connected layers\n",
        "features = None\n",
        "\n",
        "def hook(module, input, output):\n",
        "    global features\n",
        "    features = output\n",
        "\n",
        "# Create a forward hook function to capture features before the fully connected layers\n",
        "features = None\n",
        "\n",
        "def hook(module, input, output):\n",
        "    global features\n",
        "    features = output\n",
        "\n",
        "# Register the hook to the last layer\n",
        "model.feature[-1].register_forward_hook(hook)\n",
        "\n",
        "# Pass data through the model\n",
        "with torch.no_grad():\n",
        "    output = model(data)\n",
        "features = features.view(features.size(0), -1)"
      ],
      "metadata": {
        "id": "_FWzwjD447Ly"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "finding Intance and SI..."
      ],
      "metadata": {
        "id": "rw7qPb7d3vGg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of Kalhor_SeparationIndex\n",
        "instance2 = Kalhor_SeparationIndex(features, labels)\n",
        "\n",
        "# Compute the Separation Index\n",
        "si = instance2.si()\n",
        "\n",
        "# Print the Separation Index\n",
        "print(\"Separation Index:\", si.detach().cpu().numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "W8TGQhnvidMQ",
        "outputId": "415c06d1-29fc-4d90-fcd4-f97e5ba1aca4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Separation Index: 0.9794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create an instance of Kalhor_SeparationIndex\n",
        "instance2 = Kalhor_SeparationIndex(features, labels)\n",
        "\n",
        "# Compute the Separation Index\n",
        "si = instance2.si()\n",
        "\n",
        "# Print the Separation Index\n",
        "print(\"Separation Index:\", si.detach().cpu().numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3L3wqyZgxoRJ",
        "outputId": "a418c236-f9f3-4397-e8a1-7286c79c4700"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Separation Index: 0.9794\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate high-order SI with order 2\n",
        "order = 2\n",
        "high_si = instance2.high_order_si(order)\n",
        "print('High-order Separation Index (Order {}):'.format(order), high_si.detach().cpu().numpy())\n",
        "\n",
        "# Calculate high-order soft SI with order 3\n",
        "order = 3\n",
        "soft_si = instance2.soft_order_si(order)\n",
        "print('High-order Soft Separation Index (Order {}):'.format(order), soft_si.detach().cpu().numpy())\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hMH6Ir19FzPg",
        "outputId": "b7cf4bb9-b78c-4cb8-b27c-2e46f140661c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "High-order Separation Index (Order 2): 0.9642\n",
            "High-order Soft Separation Index (Order 3): 0.9746\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate center-based SI\n",
        "csi = instance2.center_si()\n",
        "print('Center-based Separation Index:', csi.detach().cpu().numpy())\n",
        "\n",
        "# Calculate anti-SI with order 2\n",
        "order = 2\n",
        "anti_si = instance2.anti_si(order)\n",
        "print('Anti Separation Index (Order {}):'.format(order), anti_si.detach().cpu().numpy())\n"
      ],
      "metadata": {
        "id": "LBcigCL9HOeh",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddd91cc4-09ef-468f-d8d9-38a5523eac0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Center-based Separation Index: 0.9518\n",
            "Anti Separation Index (Order 2): 0.0106\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "----------------------------------بخش د-----------------------------------\\\n",
        "Finding Cross SI"
      ],
      "metadata": {
        "id": "gyDa1OaO30km"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the MNIST test dataset\n",
        "mnist_dataset = torchvision.datasets.MNIST(root='./data', train=False, transform=transform, download=True)\n",
        "\n",
        "# Get the data and labels from the test set\n",
        "data = torch.stack([mnist_dataset[i][0] for i in range(len(mnist_dataset))])\n",
        "labels = torch.tensor([mnist_dataset[i][1] for i in range(len(mnist_dataset))])\n",
        "labels = labels.view(-1, 1)\n",
        "\n",
        "# Calculate cross separation index for classes\n",
        "cross_si_class = instance_concatenated.cross_si_class(data, labels)\n",
        "print(\"Cross Separation Index for Classes:\", cross_si_class.detach().cpu().numpy())\n",
        "\n",
        "# Calculate cross separation index for data points\n",
        "cr_si_data = instance_concatenated.cross_si(data, labels)\n",
        "print(\"Cross Separation Index:\", cr_si_data.detach().cpu().numpy())\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ON__-OssA8xv",
        "outputId": "0e11944a-9c21-46df-d228-5d40da8bf26f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross Separation Index for Classes: [0.99081635 0.99559474 0.9302326  0.9306931  0.9327902  0.9450673\n",
            " 0.9791232  0.9357977  0.8880904  0.9326065 ]\n",
            "Cross Separation Index: 0.9466\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "--------------------بخش و-------------------\\"
      ],
      "metadata": {
        "id": "czBEYNUCIzNR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from SmoothnessIndex import Kalhor_SmoothnessIndex\n",
        "\n",
        "# Load California housing dataset\n",
        "california_housing = fetch_california_housing()\n",
        "data = torch.tensor(california_housing.data, dtype=torch.float32, device=device)\n",
        "target = torch.tensor(california_housing.target.reshape(-1, 1), dtype=torch.float32, device=device)\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Create an instance of Kalhor_SmoothnessIndex\n",
        "housing_instance = Kalhor_SmoothnessIndex(data, target)\n",
        "\n",
        "# Calculate the Smoothness Index\n",
        "smi_linear = housing_instance.smi_linear()\n",
        "print('Linear Smoothness Index for the California housing dataset is:', smi_linear.detach().cpu().numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iqXKGkpvfmT3",
        "outputId": "13009e47-7812-4f94-8bb5-b0daaea3fdf4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Linear Smoothness Index for the California housing dataset is: 0.7351261\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from sklearn.datasets import fetch_california_housing\n",
        "from SmoothnessIndex import Kalhor_SmoothnessIndex\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Set device\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "# Load California housing dataset\n",
        "california_housing = fetch_california_housing()\n",
        "\n",
        "# Convert to PyTorch tensors\n",
        "data = torch.tensor(california_housing.data, dtype=torch.float32, device=device)\n",
        "target = torch.tensor(california_housing.target.reshape(-1, 1), dtype=torch.float32, device=device)\n",
        "\n",
        "# Split the data into training and testing sets (80% train, 20% test)\n",
        "data_train, data_test, target_train, target_test = train_test_split(data, target, test_size=0.2, random_state=42)\n",
        "\n",
        "# Create an instance of Kalhor_SmoothnessIndex\n",
        "housing_instance = Kalhor_SmoothnessIndex(data_train, target_train)\n",
        "\n",
        "# Calculate the Cross Smoothness Index\n",
        "cross_smi_linear = housing_instance.cross_smi_linear(data_test, target_test)\n",
        "print('Cross Smoothness Index for the California housing dataset is:', cross_smi_linear.detach().cpu().numpy())\n"
      ],
      "metadata": {
        "id": "qCA1ClK-wyCy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "11c7e288-d241-46e7-bf0a-f61a1ad5f336"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cross Smoothness Index for the California housing dataset is: 0.7328244\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}